# @package _global_

hydra:
  run:
    dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

common:
  seed: 48151623
  fp16: True
  memory_efficient_fp16: True
  wandb_project: ${env:WANDB_PROJECT}

distributed_training:
  heartbeat_timeout: 180

checkpoint:
  save_dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/ckpts/
  save_interval_updates: 500
  no_last_checkpoints: True
  keep_best_checkpoints: 0

dataset:
  train_subset: train_mustc,train_covost,train_dev_covost,train_test_covost
  valid_subset: dev_mustc
  num_workers: 16
  batch_size: 32
  required_batch_size_multiple: 1
  max_tokens: 1_150_000                   # ~70s @ 16kHz
  disable_validation: True

task:
  _name: speech_to_text
  data: ${env:DATA_ROOT}/en-zh
  max_source_positions: 400_000           # 25s @ 16kHz
  max_target_positions: 1024
  sampling_ratios: 1,1,1,1
  eval_gen_config:
    prefix_size: 1
  data_augmentation:
    p_augm: 0.8                           # Set 0 to deactivate
    tempo: 0.85,1.3
    pitch: 0,0
    echo_delay: 20,200
    echo_decay: 0.05,0.2

model:
  _name: s2t_pretrained

  encoder:
    path: ${env:MODELS_ROOT}/hubert/hubert_large_ll60k_finetune_ls960.pt
    layers_to_freeze:
      - r".*feature_extractor\..*|.*post_extract_proj\..*|.*pos_conv\..*"     # Feature extractor
      - r".*encoder\.layers\..*\.fc[1-2]\..*"                                 # FFN
    adapters:
      adapter_dim: 512
      adapter_scale: 4
      apply_at_self_attn: False
      apply_at_ffn: True
    length_adaptor:
      in_channels: 1024
      mid_channels: 1024
      out_channels: 1024
      kernel_sizes: [3, 3, 3]
    dropout: 0.0
    attention_dropout: 0.1
    activation_dropout: 0.0
    masking:
      apply: True
      time:
        length: 10
        prob: 0.2
      channels:
        length: 64
        prob: 0.1

  decoder:
    path: ${env:MODELS_ROOT}/mbart50.ft.1n/model.pt
    layers_to_freeze:
      - r".*embed_tokens\..*|.*embed_positions\..*|.*layernorm_embedding\..*" # Embeddings
      - r".*layers\..*\.self_attn\..*"                                        # Self-attention
      - r".*layers\..*\.fc[1-2]\..*"                                          # FFN
    adapters:
      adapter_dim: 512
      adapter_scale: 4
      apply_at_cross_attn: False
      apply_at_self_attn: True
      apply_at_ffn: True
    dropout: 0.0
    attention_dropout: 0.0
    cross_attention_dropout: 0.1
    activation_dropout: 0.0

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.2
  ignore_prefix_size: 1

optimizer:
  _name: adam
  adam_betas: (0.9, 0.98)
  adam_eps: 1e-08

optimization:
  lr: [2.5e-04]
  max_update: 33_360                      # ~16 epochs
  update_freq: [24]                       # with 1 GPU
  clip_norm: 20.0
  sentence_avg: True

lr_scheduler:
  _name: tri_stage
  phase_ratio: [0.15, 0.15, 0.7]
  init_lr_scale: 0.01
  final_lr_scale: 0.01
