# @package _global_

hydra:
  run:
    dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

common:
  seed: 48151623
  fp16: True
  memory_efficient_fp16: True
  wandb_project: ${env:WANDB_PROJECT}

distributed_training:
  find_unused_parameters: True

checkpoint:
  save_dir: ${env:SAVE_DIR}/${env:WANDB_NAME}/ckpts/
  save_interval_updates: 500
  keep_interval_updates: 10
  keep_best_checkpoints: 5

dataset:
  train_subset: train_mustc,train_covost,train_dev_covost,train_test_covost,train_europarl,train_dev_europarl,train_test_europarl
  valid_subset: dev_mustc
  num_workers: 16
  batch_size: 64
  max_tokens: 2_240_000                   # 140s @ 16kHz
  max_tokens_valid: 4_800_000             # 300s @ 16kHz
  validate_interval_updates: 500
  validate_after_updates: 7_812           # ~4 epochs
  skip_invalid_size_inputs_valid_test: True

task:
  _name: speech_to_text
  data: ${env:DATA_ROOT}/en-de
  max_source_positions: 480_000           # 30s @ 16kHz
  max_target_positions: 1024
  sampling_ratios: 1,1,1,1,1,1,1
  eval_bleu: True
  eval_gen_config:
    prefix_size: 1
  data_augmentation:
    p_augm: 0.8                           # Set 0 to deactivate
    tempo: 0.85,1.3
    pitch: 0,0
    echo_delay: 20,200
    echo_decay: 0.05,0.2
  knowledge_distillation:
    path: ${env:KD_ROOT}/en-de
    loss_lambda: 0                        # Set 0 to deactivate
    temperature: 1

model:
  _name: s2t_pretrained

  encoder:
    path: ${env:MODELS_ROOT}/wav2vec2/wav2vec_vox_960h_pl.pt
    layers_to_freeze:
      - r".*feature_extractor\..*|.*post_extract_proj\..*|.*pos_conv\..*"     # Feature extractor
      - r".*encoder\.layers\..*\.fc[1-2]\..*"                                 # FFN
    length_adaptor:
      in_channels: 1024
      mid_channels: 1024
      out_channels: 1024
      kernel_sizes: [3, 3, 3]
    dropout: 0.0
    attention_dropout: 0.1
    activation_dropout: 0.0
    masking:
      apply: True
      time:
        length: 10
        prob: 0.2
      channels:
        length: 64
        prob: 0.1

  decoder:
    path: ${env:MODELS_ROOT}/mbart50.ft.1n/model.pt
    layers_to_freeze:
      - r".*embed_tokens\..*|.*embed_positions\..*|.*layernorm_embedding\..*" # Embeddings
      - r".*layers\..*\.self_attn\..*"                                        # Self-attention
      - r".*layers\..*\.fc[1-2]\..*"                                          # FFN
    dropout: 0.0
    attention_dropout: 0.0
    cross_attention_dropout: 0.1
    activation_dropout: 0.0

criterion:
  _name: label_smoothed_cross_entropy
  label_smoothing: 0.2
  ignore_prefix_size: 1

optimizer:
  _name: adam
  adam_betas: (0.9, 0.98)
  adam_eps: 1e-08

optimization:
  lr: [1e-04]
  max_update: 31_248                      # ~16 epochs
  update_freq: [16]                       # with 1 GPU
  clip_norm: 20.0
  sentence_avg: True

lr_scheduler:
  _name: tri_stage
  phase_ratio: [0.15, 0.15, 0.7]
  init_lr_scale: 0.01
  final_lr_scale: 0.01
